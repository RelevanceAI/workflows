{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://relevance.ai/wp-content/uploads/2021/11/logo.79f303e-1.svg\" width=\"150\" alt=\"Relevance AI\" />\n",
    "<h5> Developer-first vector platform for ML teams </h5>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RelevanceAI/workflows/blob/main/workflows/vector-rake/vector_rake.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Fwj_xlgTgrD"
   },
   "source": [
    "# Installation ⚡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pnDHOaMqHjX"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install RelevanceAI==2.1.4\n",
    "!pip install rake-nltk\n",
    "!pip install -U vectorhub[clip]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa26ayJTTk2Z"
   },
   "source": [
    "# Set up 🛠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HF8HYHPx8x_T"
   },
   "source": [
    "## Variables 📑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPZ7w39o8-8r"
   },
   "outputs": [],
   "source": [
    "min_ngram_cnt = 0  # minimum number of words in a selected key-phrase\n",
    "top_n_single = 2   # number of key phrases to select from each entry\n",
    "top_n_all = 5      # number of key phrases to assign to each entry from the overall set of keyphrases\n",
    "key_word_cnt = 50  # number of key phrases to select from the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBV1R9mKWTLJ"
   },
   "source": [
    "## Client 🤖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PypLjBkvqLqb",
    "outputId": "bdd6dc91-d88f-43ae-e771-f508da5e2fd6"
   },
   "outputs": [],
   "source": [
    "from relevanceai import Client\n",
    "\n",
    "client = Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrqfDgX6TvNg"
   },
   "source": [
    "## Encoder 🦾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Akl5z4q0qNy6"
   },
   "outputs": [],
   "source": [
    "from vectorhub.encoders.text.sentence_transformers import SentenceTransformer2Vec\n",
    "\n",
    "text_model = SentenceTransformer2Vec('clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmq4id8Lmuoc"
   },
   "source": [
    "# VectorRake class 📚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPe5qjqx8pti"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from rake_nltk import Rake\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class VectorRake:\n",
    "    def __init__(self):\n",
    "        self.rake_nltk_var = Rake()\n",
    "\n",
    "    def rake_per_text(self, text:str, min_ngram_cnt:int):\n",
    "        # returns keywords extracted by Rake for each text piece\n",
    "        self.rake_nltk_var.extract_keywords_from_text(text)\n",
    "        extracted_keywords = self.rake_nltk_var.get_ranked_phrases()\n",
    "        keywords = [kw for kw in extracted_keywords if len(kw.split(' ')) >= min_ngram_cnt]\n",
    "        keywords = [kw for kw in keywords if len(kw)>2]\n",
    "        return keywords\n",
    "\n",
    "    def min_cosine_distances_text_to_keywords(self, text_vec:List, keywrds_vecs:List, top_n:int= 2):\n",
    "        # picks a subset of keywords based on the cosine distance between the doc_keyword and the doc text\n",
    "        cosine_dist = cosine_distances(keywrds_vecs, text_vec).reshape(1,-1)\n",
    "        min_dist_idxs = np.argsort(cosine_dist)[0][:top_n]\n",
    "        return min_dist_idxs\n",
    "\n",
    "    def min_cosine_distances_docs_to_keywords(self, docs_vec:List, keywrds_vecs:List, top_n:int= 5):\n",
    "        # picks a subset of all keywords for each doc, based on the coside distance\n",
    "        cosine_dist = cosine_distances(docs_vec, keywrds_vecs)\n",
    "        closest_topn_index = np.argsort(cosine_dist, axis=1)[:, :top_n]\n",
    "        return closest_topn_index\n",
    "\n",
    "    def extract_if_puntuation(self, text_list:List):\n",
    "        # returns text pieces that include punctuation\n",
    "        include_puntuation = set()\n",
    "        for text in text_list:\n",
    "            if any([True for ch in text if ch in string.punctuation]):\n",
    "                include_puntuation.add(text)\n",
    "        return include_puntuation\n",
    "\n",
    "    def extract_if_dgits(self, text_list:List):\n",
    "      # returns text pieces that include digits\n",
    "        include_digits = set()\n",
    "        for text in text_list:\n",
    "            if any([True for ch in text if ch in string.digits]):\n",
    "                include_digits.add(text)\n",
    "        return include_digits\n",
    "\n",
    "    def get_substrings(self, text_list:List):\n",
    "        # returns text pieces that are included in others \n",
    "        # e.g. \"service\" is included in \"good service\"\n",
    "        overlap = set()\n",
    "        seen_once = [itm[0] for itm in text_list.items() if itm[1]==1]\n",
    "        for h1 in seen_once[:len(seen_once)//2]:\n",
    "            for h2 in seen_once[len(seen_once)//2:]:\n",
    "                if len(h2)<len(h1):\n",
    "                    if h2 in h1:\n",
    "                        overlap.add(h2)\n",
    "                else:\n",
    "                    if h1 in h2:\n",
    "                        overlap.add(h1)\n",
    "        return overlap\n",
    "\n",
    "    def started_with_adjective(self, text_list:List):\n",
    "        # returns text pieces starting with an adjective\n",
    "        adj_list = []\n",
    "        for kw in text_list:\n",
    "            tokens_tag = pos_tag(kw.split())\n",
    "            if tokens_tag[0][1] in ['JJ', 'JJR', 'JJS']:\n",
    "                adj_list.append(kw)\n",
    "        return adj_list\n",
    "\n",
    "    def remove_plurals(self, text_list:List):\n",
    "        #todo: remove plurals from the list\n",
    "        return text_list\n",
    "\n",
    "    def process_key_words_naive(self, all_keywords:List, n:int):\n",
    "        # some basic text processing\n",
    "        all_kwrds_cnt_dict = dict(Counter(all_keywords))\n",
    "        to_skip = set()\n",
    "\n",
    "        # no punc and digits\n",
    "        to_skip = to_skip.union(self.extract_if_puntuation(all_kwrds_cnt_dict))\n",
    "        to_skip = to_skip.union(self.extract_if_dgits(all_kwrds_cnt_dict))\n",
    "\n",
    "        # repetition\n",
    "        selected_kwrds = [itm[0] for itm in all_kwrds_cnt_dict.items() if itm[1]>1]\n",
    "\n",
    "        # word overlap\n",
    "        overlap = self.get_substrings(all_kwrds_cnt_dict)\n",
    "        selected_kwrds.extend(list(overlap))\n",
    "\n",
    "        left_unused = set([itm for itm in all_kwrds_cnt_dict]) - set(selected_kwrds) - overlap - to_skip\n",
    "\n",
    "        # select adjective first\n",
    "        selected_kwrds.extend(self.started_with_adjective(left_unused))\n",
    "        left_unused -= set(selected_kwrds)\n",
    "\n",
    "        # random select\n",
    "        if len(selected_kwrds) < n:\n",
    "            if n-len(selected_kwrds)<len(left_unused):\n",
    "              selected_kwrds.extend(random.sample(left_unused, n-len(selected_kwrds)))\n",
    "        else:\n",
    "            selected_kwrds = random.sample(selected_kwrds, n)\n",
    "        \n",
    "        return selected_kwrds\n",
    "\n",
    "    def vector_rake(self, docs:List, text_model, text_field:str, \n",
    "                    vector_field:str, min_ngram_cnt:int= 0, \n",
    "                    top_n_single:int = 2,\n",
    "                    top_n_all:int = 5,\n",
    "                    n:int = 50):\n",
    "        # Updates docs with a subset of keywords selected using Rake and Cosine distance in vector space\n",
    "        \n",
    "        # key words\n",
    "        all_keywords = []\n",
    "        all_keywords_vecs = {}\n",
    "\n",
    "        for i,d in enumerate(docs):\n",
    "          if text_field in d and d[text_field] != None:\n",
    "            doc_keywords = self.rake_per_text(text = d[text_field], min_ngram_cnt = min_ngram_cnt)\n",
    "            if doc_keywords != []:\n",
    "              doc_keywrds_vecs = [text_model.encode(kw) for kw in doc_keywords]\n",
    "              min_dist_idxs = self.min_cosine_distances_text_to_keywords(\n",
    "                  text_vec = [d[vector_field]], \n",
    "                  keywrds_vecs = doc_keywrds_vecs,\n",
    "                  top_n = top_n_single)\n",
    "              all_keywords.extend([doc_keywords[idx] for idx in min_dist_idxs])\n",
    "                                \n",
    "              for j,kw in enumerate(doc_keywords):\n",
    "                all_keywords_vecs[kw]=doc_keywrds_vecs[j] \n",
    "\n",
    "        keywords_info = [{\"_id\": i,\n",
    "                            \"label\": w, \n",
    "                            \"_label_vector_\":all_keywords_vecs[w] if isinstance(all_keywords_vecs[w], List) else all_keywords_vecs[w].tolist()} \n",
    "                            for i,w in enumerate(self.process_key_words_naive(all_keywords, n))]\n",
    "\n",
    "        docs_vec = [d[vector_field] for d in docs if vector_field in d]\n",
    "        keywrds_vecs = [d[\"_label_vector_\"] for d in keywords_info]\n",
    "        closest_topn_index = self.min_cosine_distances_docs_to_keywords(docs_vec, keywrds_vecs, top_n = top_n_all)\n",
    "\n",
    "        count = 0\n",
    "        for d in docs:\n",
    "            if vector_field in d:\n",
    "                tags = []\n",
    "                for ind in closest_topn_index[count]:\n",
    "                    tags.append(keywords_info[ind][\"label\"])\n",
    "                if \"_label_\" not in d:\n",
    "                  d[\"_label_\"] = {}\n",
    "                d[\"_label_\"][text_field+\"_vector_rake\"] = tags\n",
    "                count += 1\n",
    "\n",
    "        return docs    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2omv-T4tUGTS"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Es23XuvQ-e3g"
   },
   "outputs": [],
   "source": [
    "from relevanceai.datasets import get_ecommerce_dataset_encoded\n",
    "\n",
    "docs = get_ecommerce_dataset_encoded()\n",
    "\n",
    "text_field = \"product_title\"\n",
    "vector_field='product_title_clip_vector_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REDGReNna2K3"
   },
   "source": [
    "### Label 🏷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpywA_cK354B"
   },
   "outputs": [],
   "source": [
    "vr = VectorRake()\n",
    "for d in docs:\n",
    "  if '_label_' in d:\n",
    "    del d['_label_']\n",
    "\n",
    "docs = vr.vector_rake(docs, \n",
    "                      text_model = text_model, \n",
    "                      text_field = text_field, \n",
    "                      vector_field = vector_field,\n",
    "                      n = key_word_cnt,\n",
    "                      min_ngram_cnt = min_ngram_cnt, \n",
    "                      top_n_single= top_n_single,\n",
    "                      top_n_all = top_n_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPYEZ6WJYGKg"
   },
   "source": [
    "# Sample 🟦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5jvo9cEIuUR",
    "outputId": "11ce8cc2-f922-46dc-f9ec-945ebb465b92"
   },
   "outputs": [],
   "source": [
    "d = docs[150]\n",
    "print(d[text_field])\n",
    "print(d['_label_']['product_title_vector_rake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYrBrTxCboPE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "vector-rake.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
