{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p3OFKJ7k-7t"
      },
      "source": [
        "Cluster for Papermill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LOZXlX6OQXde",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# api_key = \"\"\n",
        "# project = \"\"\n",
        "# region = \"\"\n",
        "# authorizationToken = f\"{os.environ['TEST_ACTIVATION_TOKEN']}\" if os.getenv('TEST_ACTIVATION_TOKEN') else \"\"\n",
        "\n",
        "# dataset_id = \"basic_subclustering\"\n",
        "# clusteringType = \"community-detection\"\n",
        "# cutoff = 0.75\n",
        "# encode_type = \"text\"\n",
        "# vector_fields = [\"product_title_clip_vector_\"]\n",
        "# n_clusters = 10\n",
        "\n",
        "\n",
        "token = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3hcxG57QXdf",
        "outputId": "5d107733-5ef4-4027-e35d-73f8093171f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name 'job_id' is not defined\n"
          ]
        }
      ],
      "source": [
        "# because we don't know how to set env variables\n",
        "try:\n",
        "    import os\n",
        "    os.environ[\"WORKFLOW_ID\"] = job_id\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lUjI2KJQk-7v"
      },
      "outputs": [],
      "source": [
        "\n",
        "## Instantiate client ###\n",
        "from relevanceai import Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWh1FlkNk-7v",
        "outputId": "183590d1-597e-4ef7-a094-06ecf01585ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connecting to ap-southeast-2...\n",
            "Welcome to RelevanceAI. Logged in as 6e8374136fcf2cafbbd9.\n"
          ]
        }
      ],
      "source": [
        "from relevanceai.utils import decode_workflow_token \n",
        "\n",
        "config = decode_workflow_token(token)\n",
        "authorizationToken = config['authorizationToken']\n",
        "client = Client(token=config['authorizationToken'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kOqI6i-0k-7w"
      },
      "outputs": [],
      "source": [
        "dataset_id: str = config['dataset_id']\n",
        "n_clusters: int = int(config.get('n_clusters', 100))\n",
        "clusteringType = config['clusteringType']\n",
        "vector_fields: list = config['vector_fields']\n",
        "if isinstance(vector_fields, str):\n",
        "    vector_fields = [vector_fields]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjROUZqQbiGq"
      },
      "outputs": [],
      "source": [
        "## Checking valid vector field ###\n",
        "for v in vector_fields:\n",
        "  if not '_vector_'in v:\n",
        "    raise ValueError(f\"'{v}' is not a valid vector field\")\n",
        "\n",
        "ds = client.Dataset(dataset_id)\n",
        "filters = config.get('filters', None)\n",
        "try:\n",
        "  if clusteringType == 'community-detection':\n",
        "    cluster_method  = \"community_detection\"\n",
        "    ds.cluster(\n",
        "        model=cluster_method,\n",
        "        cluster_config={\"threshold\": cutoff},\n",
        "        vector_fields=vector_fields,\n",
        "        filters=filters,\n",
        "    )\n",
        "  elif clusteringType == 'kmeans':\n",
        "    if ds.shape[0] < 10000:\n",
        "      from sklearn.cluster import KMeans\n",
        "      cluster_method = 'kmeans'\n",
        "      model = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "      alias = f\"{cluster_method}-{n_clusters}\"\n",
        "      ds.cluster(\n",
        "          model=model,\n",
        "          cluster_config={\"n_clusters\": n_clusters, \"random_state\": 42},\n",
        "          vector_fields=vector_fields,\n",
        "          alias=alias,\n",
        "          filters=filters,\n",
        "      )\n",
        "    else:\n",
        "      cluster_method = 'minibatchkmeans'\n",
        "      from sklearn.cluster import MiniBatchKMeans\n",
        "      cluster_method = 'kmeans'\n",
        "      model = MiniBatchKMeans(n_clusters=n_clusters, random_state=42)\n",
        "      alias = f\"{cluster_method}-{n_clusters}\"\n",
        "      n_clusters = int(n_clusters)\n",
        "      chunksize = n_clusters + 100\n",
        "      ds.batch_cluster(\n",
        "          model=model,\n",
        "          cluster_config={\"n_clusters\": n_clusters, \"random_state\": 42},\n",
        "          vector_fields=vector_fields,\n",
        "          alias=alias,\n",
        "          filters=filters,\n",
        "          transform_chunksize=chunksize\n",
        "      )\n",
        "  elif clusteringType == \"kmedoids\":\n",
        "    from sklearn_extra.cluster import KMedoids\n",
        "    cluster_method = \"kmedoids\"\n",
        "    model = KMedoids(n_clusters=n_clusters, random_state=42, init=\"k-medoids++\")\n",
        "    alias = f\"{cluster_method}-{n_clusters}\"\n",
        "    ds.cluster(\n",
        "        model=model,\n",
        "        vector_fields=vector_fields,\n",
        "        alias=alias,\n",
        "        filters=filters,\n",
        "    )\n",
        "except Exception as e:\n",
        "  raise ValueError(f'{e}')\n",
        "\n",
        "print(\"Finished clustering your data, you may close this window.\")\n",
        "\n",
        "ds.update_field_children(\n",
        "    field=vector_fields[0],\n",
        "    field_children=[\".\".join([\"_cluster_\", vector_fields[0], alias])],\n",
        "    category=\"cluster\"\n",
        ")\n",
        "\n",
        "client.workflows.status(\n",
        "  workflow_id=job_id,\n",
        "  status=\"complete\",\n",
        "  workflow_name=\"Clustering\",\n",
        "  metadata={},\n",
        "  additional_information=\"\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Tags",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Cluster Your Data with Relevance AI.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "33129f74f752ccabd3d30d5a3c12b8abaa2090964a9fe96b4e55e8471d1aefbe"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "name": "Cluster_Your_Data_with_Relevance_AI",
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
