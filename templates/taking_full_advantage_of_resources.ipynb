{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"\"  # @param {type:\"string\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relevanceai import Client\n",
    "\n",
    "from relevanceai.utils import decode_workflow_token\n",
    "\n",
    "config = decode_workflow_token(token)\n",
    "\n",
    "token = config[\"authorizationToken\"]\n",
    "dataset_id = config[\"dataset_id\"]\n",
    "text_field = config[\"text_field\"]\n",
    "model_name = config[\"model_name\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(token=token)\n",
    "dataset = client.Dataset(dataset_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from relevanceai.utils.doc_utils import DocUtils\n",
    "\n",
    "document = Dict[str, Any]\n",
    "documents = List[document]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Any CPU Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPUWorkflow(DocUtils):\n",
    "    \"\"\"\n",
    "    An Example Workflow that runs on the CPU.\n",
    "\n",
    "    CAN be multithreaded when transforming.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text_field: str):\n",
    "        self.text_field = text_field\n",
    "        self.new_field = f\"{text_field}_split\"\n",
    "\n",
    "    def run(self, documents: documents) -> documents:\n",
    "        \"\"\"\n",
    "        Adds a new field to each document.\n",
    "\n",
    "        This is an array of words split by ` `\n",
    "        \"\"\"\n",
    "        values = self.get_field_across_documents(\n",
    "            field=self.text_field,\n",
    "            docs=documents,\n",
    "        )\n",
    "        values = [value.split(\" \") for value in values]\n",
    "        self.set_field_across_documents(\n",
    "            field=self.new_field,\n",
    "            values=values,\n",
    "            docs=documents,\n",
    "        )\n",
    "        return documents\n",
    "\n",
    "\n",
    "cpu_workflow = CPUWorkflow(text_field=text_field)\n",
    "\n",
    "dataset.bulk_apply(    \n",
    "    bulk_func=cpu_workflow.run,\n",
    "    update_workers=8,  # sets how many threads run for udpating, can be >1 since update is on cpu\n",
    "    push_workers=8,\n",
    "    multithreaded_update=True,  # allows multiple update workers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GPU Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class GPUWorkflow(DocUtils):\n",
    "    \"\"\"\n",
    "    An Example Workflow that runs on the GPU.\n",
    "\n",
    "    CANNOT be multithreaded when transforming.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, text_field: str):\n",
    "        self.text_field = text_field\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.vector_field = f\"{model_name}_vector_\"\n",
    "\n",
    "    def run(self, documents: documents) -> documents:\n",
    "        \"\"\"\n",
    "        Adds a new field to each document.\n",
    "\n",
    "        This is vector created by `model_name` from `text_field`\n",
    "        \"\"\"\n",
    "        values = self.get_field_across_documents(\n",
    "            field=self.text_field,\n",
    "            docs=documents,\n",
    "        )\n",
    "        vectors = self.model(values)\n",
    "        self.set_field_across_documents(\n",
    "            field=self.vector_field,\n",
    "            values=vectors,\n",
    "            docs=documents,\n",
    "        )\n",
    "        return documents\n",
    "\n",
    "\n",
    "gpu_workflow = GPUWorkflow(model_name=model_name, text_field=text_field)\n",
    "\n",
    "dataset.bulk_apply(\n",
    "    bulk_func=gpu_workflow.run,\n",
    "    update_workers=1,  # must be =1 for gpu tasks\n",
    "    push_workers=8,\n",
    "    multithreaded_update=False,  # must be set false\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heavy GPU Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computationally intensive processes, hugging face models initialize certain parameters on the first model pass through. This initialisation time is dependent on the size of first batch size. Setting this to be as small as possible leads to an decrease in initialisation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs: dict  # Stuff your workflow needs for initialisation\n",
    "\n",
    "gpu_workflow = GPUWorkflow(**kwargs)\n",
    "warmup_batch_size = 1\n",
    "\n",
    "dataset.bulk_apply(\n",
    "    bulk_func=gpu_workflow.run,\n",
    "    update_workers=1,  # must be =1 for gpu tasks\n",
    "    push_workers=8,\n",
    "    multithreaded_update=False,  # must be set false\n",
    "    warmup_batch_size=1,  # this is the batch size for the first model pass through\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to Different Dataset/Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nature of the upload can be controlled directly from `PullTransformPush`. If `func` is None, the documents are simply reuploade to the new dataset. However, `func` can be any function that transforms on a list of documents. Same rules will apply to `bulk_apply` if `func` uses the GPU or CPU; these can be passed in as `kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relevanceai import Client\n",
    "\n",
    "from relevanceai.operations_new.ops_run import PullTransformPush\n",
    "\n",
    "\n",
    "old_token: str\n",
    "new_token: str\n",
    "old_dataset_id: str\n",
    "new_dataset_id: str\n",
    "\n",
    "old_client = Client(token=old_token)\n",
    "new_client = Client(token=new_token)\n",
    "\n",
    "old_dataset = old_client.Dataset(dataset_id)\n",
    "new_dataset = new_client.Dataset(dataset_id)\n",
    "\n",
    "ptp = PullTransformPush(\n",
    "    pull_dataset=old_dataset,\n",
    "    push_dataset=new_dataset,\n",
    "    func=None,\n",
    ")\n",
    "ptp.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "20b5972250e5341b38b7eabea42a1c2f8cbff13e0ba526d1b1e464dc0179bad5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
